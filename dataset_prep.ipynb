{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/mamooler/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mamooler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_dir = \"/Users/mamooler/Desktop/incontext_ie/GPT-NER/data\"\n",
    "dataset_name = \"bc5disease\" # \"conll2003\"\n",
    "entity_type = \"DISEASE\" # \" PER\" # \" ORG\" # \" LOC\" # \" MISC\"\n",
    "entity_type2def = {\n",
    "    \"CHEMICAL\": \"are substances with a distinct molecular composition that are produced by or used in a chemical process. They can be elements or compounds, and they may exist in various forms—solid, liquid, or gas\",\n",
    "    \"DISEASE\": \"are abnormal conditions or disorders of a structure or function in a living organism, often associated with specific signs and symptoms.\",\n",
    "    \"GENE\": \"are specific sequence of nucleotides within a DNA molecule that encode information for the synthesis of a functional product, such as a protein or functional RNA. A gene is typically mentioned with reference to its associated traits, functions, or the role it plays in a biological process\"\n",
    "}\n",
    "conll_task2entity_type = {\"person entities\": 'PER', \"organization entities\": 'ORG', \"location entities\": 'LOC', \"miscellaneous entities\": 'MISC'}\n",
    "\n",
    "dataset_output_dir = \"/\".join([output_data_dir, dataset_name])\n",
    "os.makedirs(dataset_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"chemprot\" in dataset_name:\n",
    "    dataset_train = load_dataset(\"bigbio/chemprot\", \"chemprot_full_source\", split=\"train\")\n",
    "    dataset_val = load_dataset(\"bigbio/chemprot\", \"chemprot_full_source\", split=\"validation\")\n",
    "    dataset_test = load_dataset(\"bigbio/chemprot\", \"chemprot_full_source\", split=\"test\")\n",
    "elif \"conll\" in dataset_name:\n",
    "    dataset_train = load_dataset(dataset_name, split=\"train\")\n",
    "    dataset_val = load_dataset(dataset_name, split=\"validation\")\n",
    "    dataset_test = load_dataset(dataset_name, split=\"test\")\n",
    "else:\n",
    "    dataset_train = load_dataset(\"bigbio/blurb\", dataset_name, split=\"train\")\n",
    "    dataset_val = load_dataset(\"bigbio/blurb\", dataset_name, split=\"validation\")\n",
    "    dataset_test = load_dataset(\"bigbio/blurb\", dataset_name, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_token_spans(tags, entity_type, dataset_name):\n",
    "    \"\"\"\n",
    "    convert BIO format to a list of start and end indices for entities\n",
    "    for the blurb benchmark dataset, the tags are {0:O, 1:B, 2:I}\n",
    "    \"\"\"\n",
    "    conll_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "    tags_dict = {'O':0, 'B':1, 'I':2}\n",
    "    if \"conll\" in dataset_name:\n",
    "        tags_dict = {'O': 0, 'B': conll_tags['B-'+entity_type], 'I': conll_tags['I-'+entity_type]}\n",
    "    spans = []\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == tags_dict['B']:\n",
    "            start = i\n",
    "        elif tag == tags_dict['I']:\n",
    "            pass\n",
    "        elif tag == tags_dict['O']:\n",
    "            if start != -1:\n",
    "                end = i\n",
    "                spans.append((start, end-1))\n",
    "                start = -1\n",
    "                end = -1\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_span_to_char_span(tokens, token_span):\n",
    "    \"\"\"\n",
    "    convert token span to char span\n",
    "    \"\"\"\n",
    "    start = token_span[0]\n",
    "    end = token_span[1]\n",
    "    # if the start token is the first token, then the start char is 0 otherwise it's the length of the tokens before it plus a space\n",
    "    start_char = len(\" \".join(tokens[:start])) + min(start, 1)\n",
    "    end_char = len(\" \".join(tokens[start:end+1])) + start_char - 1\n",
    "    return (start_char, end_char)\n",
    "\n",
    "### test the token_span_to_char_span function\n",
    "# token_span_test = [0,0]\n",
    "# text_test = \"start with bc5 dataset\"\n",
    "# tokens_test = text_test.split()\n",
    "# char_span_test = token_span_to_char_span(tokens_test, token_span_test)\n",
    "# start = char_span_test[0]\n",
    "# end = char_span_test[1]\n",
    "# print(text_test[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsets_to_char_span(offsets):\n",
    "    \"\"\"\n",
    "    convert offsets to char span\n",
    "    \"\"\"\n",
    "    spans = [(start, end-1) for start, end in offsets]\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_to_gpt_ner_format(hf_dataset, dataset_name, entity_type, entity_def):\n",
    "    \"\"\"\n",
    "    convert a huggingface dataset to the GPT-NER format\n",
    "    \"\"\"\n",
    "    gpt_ner_dataset = []\n",
    "    \n",
    "    if \"chemprot\" in dataset_name:\n",
    "        for i, text in enumerate(tqdm(hf_dataset['text'])):\n",
    "            annotations = hf_dataset['entities'][i]\n",
    "            types = annotations['type']\n",
    "            offsets = [o for i,o in enumerate(annotations['offsets']) if types[i] == entity_type]\n",
    "            char_spans = offsets_to_char_span(offsets)\n",
    "            gpt_ner_dataset.append(\n",
    "                {\n",
    "                    'context': text,\n",
    "                    'end_position_char': [s[1] for s in char_spans],\n",
    "                    'entity_label': entity_type,\n",
    "                    'impossible': len(char_spans) == 0,\n",
    "                    'qas_id': f\"{i}.0\",\n",
    "                    'query': entity_def,\n",
    "                    'span_position_char': [f\"{s[0]};{s[1]}\" for s in char_spans],\n",
    "                    'start_position_char': [s[0] for s in char_spans],\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        for i, tokens in enumerate(tqdm(hf_dataset['tokens'])):\n",
    "            text = \" \".join(tokens)\n",
    "            tags = hf_dataset['ner_tags'][i]\n",
    "            word_spans = bio_to_token_spans(tags, entity_type, dataset_name)\n",
    "            char_spans = [token_span_to_char_span(tokens, word_span) for word_span in word_spans]\n",
    "            gpt_ner_dataset.append(\n",
    "                {\n",
    "                    'context': text,\n",
    "                    'end_position_word': [s[1] for s in word_spans],\n",
    "                    'end_position_char': [s[1] for s in char_spans],\n",
    "                    'entity_label': entity_type,\n",
    "                    'impossible': len(word_spans) == 0,\n",
    "                    'qas_id': f\"{i}.0\",\n",
    "                    'query': entity_def,\n",
    "                    'span_position_word': [f\"{s[0]};{s[1]}\" for s in word_spans],\n",
    "                    'span_position_char': [f\"{s[0]};{s[1]}\" for s in char_spans],\n",
    "                    'start_position_word': [s[0] for s in word_spans],\n",
    "                    'start_position_char': [s[0] for s in char_spans],\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    return gpt_ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12574/12574 [18:34<00:00, 11.28it/s]\n"
     ]
    }
   ],
   "source": [
    "gpt_ner_train = hf_to_gpt_ner_format(dataset_train, dataset_name, entity_type, entity_type2def[entity_type])\n",
    "gpt_ner_val = hf_to_gpt_ner_format(dataset_val, dataset_name, entity_type, entity_type2def[entity_type])\n",
    "gpt_ner_test = hf_to_gpt_ner_format(dataset_test, dataset_name, entity_type, entity_type2def[entity_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for char and word indices\n",
    "def check_char_word_span(dataset):\n",
    "    i = 0\n",
    "    for sample in dataset:\n",
    "        i += 1\n",
    "        text = sample['context']\n",
    "        tokens = text.split()\n",
    "        assert len(sample['start_position_char']) == len(sample['start_position_word']), \"start_position_char and start_position_word should have the same length\"\n",
    "        assert len(sample['end_position_char']) == len(sample['end_position_word']), \"end_position_char and end_position_word should have the same length\"\n",
    "        for span_index, char_span in enumerate(sample['span_position_char']):\n",
    "            start_char = int(char_span.split(\";\")[0])\n",
    "            end_char = int(char_span.split(\";\")[1])\n",
    "            span_word = sample['span_position_word'][span_index]\n",
    "            start_word = int(span_word.split(\";\")[0])\n",
    "            end_word = int(span_word.split(\";\")[1])\n",
    "            char_span_text = text[start_char:end_char+1]\n",
    "            word_span_text = \" \".join(tokens[start_word:end_word+1])\n",
    "            assert char_span_text == word_span_text, f\"the span {text[start_char:end_char+1]} should be equal to {' '.join(tokens[start_word:end_word+1])}\"\n",
    "\n",
    "if \"chemprot\" not in dataset_name:\n",
    "    # check_char_word_span(gpt_ner_train)\n",
    "    check_char_word_span(gpt_ner_val)   \n",
    "    check_char_word_span(gpt_ner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(gpt_ner_train, open(f\"{dataset_output_dir}/ner.train\", \"w\"), indent=4, ensure_ascii=False)\n",
    "json.dump(gpt_ner_val, open(f\"{dataset_output_dir}/ner.val\", \"w\"), indent=4, ensure_ascii=False)\n",
    "json.dump(gpt_ner_test, open(f\"{dataset_output_dir}/ner.test\", \"w\"), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_ner_test_100 = []\n",
    "texts = [e[\"text\"] for e in json.load(open(f\"/Users/mamooler/Desktop/incontext_ie/ICL_IE/data/{dataset_name}/test_100samples.json\"))]\n",
    "\n",
    "# preserve the order of the samples\n",
    "for text in texts:\n",
    "    for sample in gpt_ner_test:\n",
    "        if sample[\"context\"] == text:\n",
    "            gpt_ner_test_100.append(sample)\n",
    "            \n",
    "json.dump(gpt_ner_test_100, open(f\"{dataset_output_dir}/ner.test.100\", \"w\"), indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
